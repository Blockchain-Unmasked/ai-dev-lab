#!/usr/bin/env python3
"""
Test script for Agent Prompting Strategy
Demonstrates the integration of contact center research with Gemini prompting best practices
"""

import json
import asyncio
from pathlib import Path
from agent_prompting_strategy import AgentPromptingEngine, AgentTier, InteractionType

async def test_prompting_strategy():
    """Test the agent prompting strategy with various scenarios"""
    
    print("ü§ñ AI/DEV Lab - Agent Prompting Strategy Test")
    print("=" * 60)
    
    # Initialize the prompting engine
    engine = AgentPromptingEngine()
    
    # Test scenarios
    test_scenarios = [
        {
            "name": "Tier 1 - Basic Password Reset",
            "tier": AgentTier.TIER_1,
            "interaction_type": InteractionType.BASIC_INQUIRY,
            "customer_message": "Hi, I forgot my password and need to reset it",
            "expected_escalation": False
        },
        {
            "name": "Tier 1 - Complex API Issue (Should Escalate)",
            "tier": AgentTier.TIER_1,
            "interaction_type": InteractionType.TECHNICAL_SUPPORT,
            "customer_message": "I'm having trouble with your API integration and need help with OAuth authentication and webhook configuration",
            "expected_escalation": True
        },
        {
            "name": "Tier 2 - Technical Support",
            "tier": AgentTier.TIER_2,
            "interaction_type": InteractionType.TECHNICAL_SUPPORT,
            "customer_message": "I need help configuring the API endpoints for my application",
            "expected_escalation": False
        },
        {
            "name": "Tier 3 - VIP Customer Issue",
            "tier": AgentTier.TIER_3,
            "interaction_type": InteractionType.VIP_SUPPORT,
            "customer_message": "I'm a premium customer and need immediate assistance with a critical system issue",
            "expected_escalation": False
        }
    ]
    
    for i, scenario in enumerate(test_scenarios, 1):
        print(f"\nüìã Test {i}: {scenario['name']}")
        print("-" * 40)
        
        # Generate prompt
        prompt = engine.generate_agent_prompt(
            scenario["tier"],
            scenario["interaction_type"],
            scenario["customer_message"]
        )
        
        print(f"üéØ Tier: {scenario['tier'].value.upper()}")
        print(f"üí¨ Customer Message: {scenario['customer_message']}")
        print(f"üìù Prompt Length: {len(prompt)} characters")
        
        # Check escalation decision
        escalation_decision = engine.get_escalation_decision(
            scenario["tier"],
            scenario["customer_message"],
            {"customer_type": "standard"}
        )
        
        print(f"üîÑ Escalation Needed: {escalation_decision['escalate']}")
        if escalation_decision['escalate']:
            print(f"üìà Reason: {escalation_decision['reason']}")
            print(f"‚¨ÜÔ∏è  Recommended Tier: {escalation_decision['recommended_tier'].value}")
        
        # Show prompt preview (first 300 characters)
        print(f"üìÑ Prompt Preview:")
        print(f"   {prompt[:300]}...")
        
        # Verify escalation expectation
        if escalation_decision['escalate'] == scenario['expected_escalation']:
            print("‚úÖ Escalation decision matches expectation")
        else:
            print("‚ö†Ô∏è  Escalation decision doesn't match expectation")
    
    print(f"\nüéâ Testing Complete!")
    print("=" * 60)

async def test_mcp_integration():
    """Test MCP server integration"""
    
    print("\nüîå Testing MCP Server Integration")
    print("-" * 40)
    
    try:
        from agent_prompting_mcp_server import AgentPromptingMCP
        
        mcp = AgentPromptingMCP()
        
        # Test prompt generation
        result = mcp.generate_prompt(
            "tier_1",
            "basic_inquiry",
            "I need help with my account",
            {"customer_type": "standard"}
        )
        
        if result["success"]:
            print("‚úÖ MCP prompt generation successful")
            print(f"   Prompt length: {result['prompt_length']} characters")
            print(f"   Estimated tokens: {result['tokens_estimated']:.0f}")
        else:
            print(f"‚ùå MCP prompt generation failed: {result['error']}")
        
        # Test escalation check
        escalation = mcp.check_escalation(
            "tier_1",
            "I have a complex technical issue with API integration",
            {"customer_type": "enterprise"}
        )
        
        if escalation["success"]:
            print("‚úÖ MCP escalation check successful")
            decision = escalation["escalation_decision"]
            print(f"   Escalation needed: {decision['escalate']}")
            if decision['escalate']:
                print(f"   Reason: {decision['reason']}")
        else:
            print(f"‚ùå MCP escalation check failed: {escalation['error']}")
        
        # Test capabilities retrieval
        capabilities = mcp.get_agent_capabilities("tier_1")
        
        if capabilities["success"]:
            print("‚úÖ MCP capabilities retrieval successful")
            caps = capabilities["capabilities"]
            print(f"   Responsibilities: {len(caps['responsibilities'])} items")
            print(f"   Tools available: {len(caps['tools_available'])} items")
            print(f"   Escalation triggers: {len(caps['escalation_triggers'])} items")
        else:
            print(f"‚ùå MCP capabilities retrieval failed: {capabilities['error']}")
            
    except ImportError as e:
        print(f"‚ö†Ô∏è  MCP integration test skipped: {e}")
        print("   (MCP dependencies not installed)")

async def test_quality_metrics():
    """Test quality metrics and evaluation"""
    
    print("\nüìä Testing Quality Metrics")
    print("-" * 40)
    
    engine = AgentPromptingEngine()
    
    # Test quality metrics for different tiers
    for tier in [AgentTier.TIER_1, AgentTier.TIER_2, AgentTier.TIER_3]:
        metrics = engine.get_quality_metrics(tier)
        
        print(f"üéØ {tier.value.upper()} Quality Metrics:")
        print(f"   Evaluation Criteria: {len(metrics['evaluation_criteria'])} items")
        print(f"   Scoring System: {metrics['scoring_system']}")
        print(f"   Monitoring Frequency: {metrics['frequency']}")
        
        # Show first few criteria
        criteria = metrics['evaluation_criteria'][:3]
        print(f"   Sample Criteria: {', '.join(criteria)}")

async def test_research_integration():
    """Test integration with contact center research"""
    
    print("\nüìö Testing Research Integration")
    print("-" * 40)
    
    engine = AgentPromptingEngine()
    
    if engine.research_data:
        print("‚úÖ Research data loaded successfully")
        
        # Check research components
        components = list(engine.research_data.keys())
        print(f"   Research Components: {len(components)}")
        for component in components:
            print(f"   - {component}")
        
        # Check agent capabilities
        capabilities_count = len(engine.agent_capabilities)
        print(f"   Agent Tiers Configured: {capabilities_count}")
        
        for tier, caps in engine.agent_capabilities.items():
            print(f"   - {tier.value}: {len(caps.responsibilities)} responsibilities")
    else:
        print("‚ö†Ô∏è  No research data loaded")
        print("   (Research files not found or not accessible)")

async def main():
    """Main test function"""
    
    print("üöÄ Starting Agent Prompting Strategy Tests")
    print("=" * 60)
    
    # Run all tests
    await test_prompting_strategy()
    await test_mcp_integration()
    await test_quality_metrics()
    await test_research_integration()
    
    print(f"\nüéâ All Tests Completed!")
    print("=" * 60)
    
    # Show implementation summary
    print("\nüìã Implementation Summary:")
    print("‚úÖ Agent Prompting Strategy Engine")
    print("‚úÖ Contact Center Research Integration")
    print("‚úÖ Gemini Prompting Best Practices")
    print("‚úÖ MCP Server Architecture")
    print("‚úÖ Tier-Based Agent System")
    print("‚úÖ Quality Assurance Metrics")
    print("‚úÖ Escalation Decision Making")
    
    print(f"\nüöÄ Ready for Production Deployment!")

if __name__ == "__main__":
    asyncio.run(main())
